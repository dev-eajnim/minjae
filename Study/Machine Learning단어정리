▶ 인공지능
→ 기계를 지능적으로 만드는 과학
→ AI 알고리즘은 자체 규칙 시스템을 구축

▶ 머신러닝
→ 정확한 결정을 내리기 위해 제공된 데이터를 통하여 스스로 학습할 수 있는 것입니다.
→ 빅데이터를 통한 학습 방법으로 머신러닝을 이용할 수 있습니다. 
→ 머신 러닝은 기본적으로 알고리즘을 이용해 데이터를 분석하고, 분석을 통해 학습하며, 학습한 내용을 기반으로 판단이나 예측을 합니다.
→ 의사 결정 기준에 대한 구체적인 지침을 소프트웨어에 직접 코딩해 넣는 것이 아닌, 
  대량의 데이터와 알고리즘을 통해 컴퓨터 그 자체를 ‘학습’시켜 작업 수행 방법을 익히는 것을 목표로 합니다.

▶ 딥러닝
→ 인공신경망에서 발전한 형태의 인공 지능
→ 뇌의 뉴런과 유사한 정보 입출력 계층을 활용해 데이터를 학습합니다.
→ Ex) 알파고, 종양식별 능력
(정확도 민감도 특이도 정밀도 재현율 을 다 따져서 딥러닝이 되는 것)

▷ 가장 큰 차이점은... 
딥러닝은 분류에 사용할 데이터를 스스로 학습할 수 있는 반면 
머신러닝은 학습 데이터를 수동으로 제공해야한다는점이 딥러닝과 머신러닝의 가장 큰 차이점입니다.

▷ 인공지능:가장 넓음 ) 머신러닝 ) 딥러닝 - 가장 좁음

▶ 신경망의 정확도 95%는 얼마나 정확한 것일까요?
→ 우리의 신경망은 95% 정확하다 할 수 있습니다.
  95%라는 것은 다른 의미로 볼 수도 있습니다
  
→ 만약 훈련 이미지의 5 %가 새이고 나머지 95%가 새가 아닌 경우 어떻게 될까요? 
  매번 “새가 아니다”라고 추측하는 프로그램이 있다면 이것은 95% 정확한 것이 됩니다! 
  그러나 이것은 또한 100% 쓸모 없습니다.
  
  1) 맞힌 긍정(true positives): 신경망이 올바르게 새를 식별함 (여러 종류의 새를 성공적으로 인식)
  2) 맞힌 부정(true negatives): '새가 아니다'라고 올바르게 식별 (말과 트럭은 새가 아니라고 인식)
  3) 틀린 긍정(false positives): 새라고 생각했지만, 실제 새가 아닌 이미지 (비행기를 새라고 착각한 경우)
  4) 틀린 부정(false negatives): 정확하게 새로 인식하지 못한 이미지 (이 새들에게 속았습니다. 타조도 새인가요?)
  → 평균적인 정확도를 보는 대신 precision 과 recall을 계산했습니다.
    precision: If we predicted bird, how often was it really a bird? 
    recall: What percentage of the actual birds did we find?
    예) precision = 97%. recall = 90% 라면, 
        97%의 정확도로 '새'를 추측했다. 그러나 데이터 셋에서 실제 새의 90%만 발견했다.
        모든 새를 발견하지 못할 수는 있지만, 발견했을 때는 꽤 정확하게 맞출 수 있다.

▶ epoch
: One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE

→ 한 번의 epoch는 인공 신경망에서 전체 데이터 셋에 대해 forward pass/backward pass 과정을 거친 것을 말함. 
  즉, [전체 데이터 셋에 대해 한 번 학습을 완료한 상태]

→ 신경망에서 사용되는 역전파 알고리즘(backpropagation algorithm)은 
  파라미터를 사용하여 입력부터 출력까지의 각 계층의 weight를 계산하는 과정을 거치는 순방향 패스(forward pass), 
  forward pass를 반대로 거슬러 올라가며 다시 한 번 계산 과정을 거처 기존의 weight를 수정하는 역방향 패스(backward pass)로 나뉩니다. 
  이 전체 데이터 셋에 대해 해당 과정(forward pass + backward pass)이 완료되면 한 번의 epoch가 진행됐다고 볼 수 있습니다.

→ epochs = 40이라면 전체 데이터를 40번 사용해서 학습을 거치는 것입니다.

→ 우리는 모델을 만들 때 적절한 epoch 값을 설정해야만 underfitting과 overfitting을 방지할 수 있습니다.
  epoch 값이 너무 작다면 underfitting이 
             너무 크다면 overfitting이 발생할 확률이 높은 것이죠.
             
▶ batch size
: Total number of training examples present in a single batch.

→ 한 번의 batch 마다 주는 데이터 샘플의  size

▶ batch
: 보통 mini batch라고 함
→ 나눠진 데이터 셋

▶ iteration
: The number of passes to complete one epoch.

→ epoch를 나누어서 실행하는 횟수

▷ 정리...
메모리의 한계와 속도 저하때문에 대부분의 경우에는 한 번의 epoch 에서 모든 데이터를 한꺼번에 집어넣을 수는 없습니다. 
그래서 데이터를 나누어서 주게 되는데 이 때!
  몇 번 나누어서 주는가를 iteration,
  각 iteration마다 주는 데이터 사이즈를 batch size라고 합니다. 
  
▷ 문제...
1) 
총 데이터 = 100
batch size = 10

그럼 1iteration = 10, 10개 데이터에 대해서 학습
1 epoch = 100/batch size = 10 iteratoin

2) 
총 데이터 = 2000
epochs = 20
batch size = 500

그럼 1iteration = 4

1epoch는 데이터 size가 500인 batch가 4번의 iteration으로 나누어집니다.

전체 데이터 셋을 20번 학습하므로
iteration입장에서는 80번 학습이 이루어진 것이다.

※issues에 사진 참고!


★ https://blog.naver.com/einbot/221851269475
와 얘가 인공지닝 딥러닝 비전 설명이며 코드를 잘 정리해놨다. 
