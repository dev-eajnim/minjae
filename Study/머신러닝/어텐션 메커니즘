▶ '어텐션 메커니즘'이란?
  기계 번역 분야에서 입력 문장이 길면 번역 품질이 떨어지게 됩니다. 
  이를 위한 대안으로 입력 시퀀스가 길어지면 출력 시퀀스의 정확도가 떨어지는 것을 보정해주기 위한 등장한 기법인 '어텐션'(attention)

  디코더에서 출력 단어를 예측하는 매 시점(time step)마다, 인코더에서의 전체 입력 문장을 다시 한 번 참고한다.
  전체 입력 문장을 전부 다 동일한 비율로 참고하는 것이 아니라, 
  해당 시점에서 예측해야할 단어와 연관이 있는 입력 단어 부분을 좀 더 집중(attention)한다.

▶ 파이썬의 딕셔너리 자료형을 선언
  # 키(Key) : 값(value)의 형식으로 키와 값의 쌍(Pair)을 선언한다.
  dict = {"2017" : "Transformer", "2018" : "BERT"}

  print(dict["2017"]) #2017이라는 키에 해당되는 값을 출력: Transformer
  print(dict["2018"])  #2018이라는 키에 해당되는 값을 출력: BERT

▶ 어텐션을 함수로 표현
  Q = Query : t 시점의 디코더 셀에서의 은닉 상태
  K = Keys : 모든 시점의 인코더 셀의 은닉 상태들
  V = Values : 모든 시점의 인코더 셀의 은닉 상태들
  
  주어진 '쿼리(Query)'에 대해서 모든 '키(Key)'와의 유사도를 각각 구합니다. 
  그리고 구해낸 이 유사도를 키와 맵핑되어있는 각각의 '값(Value)'에 반영해줍니다. 
  그리고 유사도가 반영된 '값(Value)'을 모두 더해서 리턴합니다. 
  여기서는 이를 어텐션 값(Attention Value)이라고 하겠습니다.
  
  query가 각각의 key에 어느정도 attention을 둬야하는지 계산한다.
  key와 value는 attention이 이루어지는 위치에 상관없이 같은 값을 가진다.
  
  query와 key에 대한 dot-product(내적)를 계산하면 각각의 query와 key 사이의 유사도를 구할 수 있다.
  이 후 softmax를 거친 값을 value에 곱해줄 때, query와 유사한 value일수록, 즉 중요한 value일수록 더 높은 값을 가지게 된다.
  
  q1 = x1 * Wq, k1 = x1 * Wk, v1 = x1 * Wv
