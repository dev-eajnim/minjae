▶시퀀스-투-시퀀스(Sequence-to-Sequence)
  : 입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력하는 다양한 분야에서 사용되는 모델입니다.
    ex) 챗봇, 기계번역, 내용요약(text summarization), stt(speech to text)
    
▶ 내부 구성 및 과정
  - seq2seq는 크게 두 개로 구성된 아키텍처로 구성되는데, 바로 인코더와 디코더입니다.
  - 인코더는 입력 문장의 모든 단어들을 순차적으로 입력받은 뒤에
    마지막에 이 모든 단어 정보들을 압축해서 하나의 벡터로 만드는데, 
    이를 컨텍스트 벡터(context vector)라고 합니다. 
  - 입력 문장의 정보가 하나의 컨텍스트 벡터로 모두 압축되면 인코더는 컨텍스트 벡터를 디코더로 전송합니다. 
  - 디코더는 컨텍스트 벡터를 받아서 번역된 단어를 한 개씩 순차적으로 출력합니다.

▶ (RNN을 사용하기에) RNN(current neural network,순환 신경망)이란
  - RNN은 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 출력층 방향으로도 보내면서, 
    다시 은닉층 노드의 다음 계산의 입력으로 보내는 특징을 갖고있습니다
    Xt -----> Cell -----> Yt 
    X: 입력층의 입력 벡터
    Y: 출력층의 출력 벡터
    Cell: 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드, 이전의 값을 기억하는 메모리 역할 수행 
          바로 이전 시점에서의 은닉층의 메모리 셀에서 나온 값을 자신의 입력으로 사용하는 재귀적 활동을 함
    t: 현재 시점, 현재 시점은 과거의 메모리 셀들의 값에 영향을 받은 것임
  - 메모리 셀(셀과 같음)이 출력층 방향으로 or 다음 시점 t+1의 자신에게 보내는 값을 은닉상태라고 함
  - t 시점의 메모리 셀은 t-1 시점의 메모리 셀이 보낸 은닉 상태값을 t시점의 은닉 상태 계산을 위한 입력값으로 사용
  - 하나의 RNN 셀은 각각의 시점(time step)마다 두 개의 입력을 받습니다. 
  - RNN 층은 (batch_size, timesteps, input_dim) 크기의 3D 텐서를 입력으로 받습니다. 

▶ 한계
    바닐라 RNN은 출력 결과가 이전의 계산 결과에 의존한다는 것을 언급한 바 있습니다. 
    하지만 바닐라 RNN은 비교적 짧은 시퀀스(sequence)에 대해서만 효과를 보이는 단점이 있습니다. 
    바닐라 RNN의 시점(time step)이 길어질 수록 앞의 정보가 뒤로 충분히 전달되지 못하는 현상이 발생합니다. 
    위의 그림은 첫번째 입력값인 x1의 정보량을 짙은 남색으로 표현했을 때, 
    색이 점차 얕아지는 것으로 시점이 지날수록 x1의 정보량이 손실되어가는 과정을 표현하였습니다. 
    뒤로 갈수록 x1의 정보량은 손실되고, 시점이 충분히 긴 상황에서는 x1의 전체 정보에 대한 영향력은 거의 의미가 없을 수도 있습니다.

    어쩌면 가장 중요한 정보가 시점의 앞 쪽에 위치할 수도 있습니다. 
    RNN으로 만든 언어 모델이 다음 단어를 예측하는 과정을 생각해봅시다. 
    예를 들어 ''모스크바에 여행을 왔는데 건물도 예쁘고 먹을 것도 맛있었어. 
    그런데 글쎄 직장 상사한테 전화가 왔어. 어디냐고 묻더라구 그래서 나는 말했지. 저 여행왔는데요. 여기 ___'' 
    다음 단어를 예측하기 위해서는 장소 정보가 필요합니다. 
    그런데 장소 정보에 해당되는 단어인 '모스크바'는 앞에 위치하고 있고, 
    RNN이 충분한 기억력을 가지고 있지 못한다면 다음 단어를 엉뚱하게 예측합니다.
    
    이를 장기 의존성 문제(the problem of Long-Term Dependencies)라고 합니다.
    
▶보안한 것이 LSTM(RNN의 일종을 장단기 메모리(Long Short-Term Memory))
  - LSTM은 은닉층의 메모리 셀에 입력 게이트, 망각 게이트, 출력 게이트를 추가하여 불필요한 기억을 지우고, 기억해야할 것들을 정합니다.
  - 요약하면 LSTM은 은닉 상태(hidden state)를 계산하는 식이 전통적인 RNN보다 조금 더 복잡해졌으며 
    셀 상태(cell state)라는 값을 추가하였습니다. 위의 그림에서는 t시점의 셀 상태를 Ct로 표현하고 있습니다. 
    LSTM은 RNN과 비교하여 긴 시퀀스의 입력을 처리하는데 탁월한 성능을 보입니다.

참고) https://wikidocs.net/22886
