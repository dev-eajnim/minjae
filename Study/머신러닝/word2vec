▶ 워드투벡터(word2vec) 
- 단어 간 유사도를 반영할 수 있도록 단어의 의미를 벡터화 할 수 있는 방법

▶ 분산 표현(distributed representation)
- 단어의 '의미'를 다차원 공간에 벡터화하는 방법
- 가정: '비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다'
- 강아지란 단어는 귀엽다, 예쁘다, 애교 등의 단어가 주로 함께 등장하는데 
  분포 가설에 따라서 저런 내용을 가진 텍스트를 벡터화한다면 저 단어들은 의미적으로 가까운 단어가 됩니다. 
- 희소 표현이 고차원에 각 차원이 분리된 표현 방법이었다면, 분산 표현은 저차원에 단어의 의미를 여러 차원에다가 분산하여 표현합니다. 
  이런 표현 방법을 사용하면 "단어 간 유사도"를 계산할 수 있습니다.
  
  ex) '강아지' 라는 단어의 인덱스가 5라면 
       <희소표현> 강아지 = [0,0,0,0,1,~중략,0]
       <분산표현> 강아지 = [0.2 0.3 0.5 0.7 0.2 ~중략~, 0.2] 
                           [귀엽다, 예쁘다, 애교 ,...]
                           
- 이를 위한 학습 방법으로는 NNLM, RNNLM 등이 있으나 요즘에는 해당 방법들의 속도를 대폭 개선시킨 Word2Vec가 많이 쓰이고 있습니다.

Word2Vec에는 CBOW(Continuous Bag of Words)와 Skip-Gram 두 가지 방식이 있습니다
▶ CBOW(Continuous Bag of Words)
- 주변에 있는 단어들을 가지고, 중간에 있는 단어들을 예측하는 방법

▶ Skip-Gram
- 중간에 있는 단어로 주변 단어들을 예측하는 방법
